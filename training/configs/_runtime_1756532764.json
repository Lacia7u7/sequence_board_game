{
  "rules": {
    "mode": "1v1",
    "teams": 2,
    "players_per_team": 1,
    "hand_size": 7,
    "allowAdvancedJack": false,
    "win_sequences_needed": 2,
    "reset_full_board_no_winner": false
  },
  "engine": {
    "use_cuda_sequences": true,
    "reshuffle_on_empty_deck": true
  },
  "observation": {
    "use_lstm": true,
    "history_k": 10,
    "channels": {
      "team_chips": true,
      "playable_mask_self": true,
      "team_sequences": true,
      "corner_mask": true,
      "static_card_17ch": true,
      "threats_len4_opponent": true,
      "extendable_len3_self": true,
      "jack_remove_targets": true,
      "p_cell_playable_next": true,
      "p_cell_playable_k": [
        2,
        3,
        4,
        5,
        6
      ],
      "p_opponent_targets_next": true
    }
  },
  "action_space": {
    "type": "unified",
    "max_hand": 7,
    "include_pass": false
  },
  "training": {
    "algorithm": "ppo_lstm",
    "seed": 123,
    "num_envs": 8,
    "rollout_length": 64,
    "total_updates": 100,
    "epochs": 6,
    "gamma": 0.995,
    "gae_lambda": 0.95,
    "lr": 0.00015,
    "clip_eps": 0.2,
    "entropy_coef": 0.01,
    "value_coef": 0.5,
    "max_grad_norm": 0.5,
    "amp": true,
    "minibatch_size": 128,
    "episode_cap": 200,
    "snapshot_every": 100,
    "max_snapshots": 30,
    "pool_probabilities": {
      "current": 0.0,
      "snapshots": 0.55,
      "heuristics": 0.45
    },
    "resume_from_latest_run": true,
    "resume_load_strict": false
  },
  "model": {
    "conv_channels": [
      32,
      64,
      128,
      128
    ],
    "lstm_hidden": 128,
    "lstm_layers": 2,
    "value_tanh_bound": 5.0
  },
  "logging": {
    "tensorboard": true,
    "csv": true,
    "jsonl": true,
    "log_private_hands_locally": true,
    "logdir": "runs/smoke/full"
  },
  "evaluation": {
    "elo": true,
    "trueskill": true,
    "eval_interval_updates": 2,
    "opponent_snapshots_keep": 2,
    "benchmark_agents": [
      "agents/baseline/random_agent.py"
    ],
    "evaluated_agents": [
      "agents/training/ppo_lstm_agent.py"
    ],
    "teams": 1,
    "agents_per_team": 1,
    "agent_kwargs": {
      "agents/training/ppo_lstm_agent.py": {
        "policy_path": "E:/sequence_game_board/sequence_board_game/training/runs/smoke/full/run/policy_final.pt",
        "deterministic": true
      }
    }
  },
  "rewards": {
    "illegal": -0.01,
    "win": 1.0,
    "loss": -1.0,
    "seq_bonus": 0.25,
    "step_penalty": 0.0,
    "open4_self": 0.03,
    "open4_opp": 0.03,
    "open3_self": 0.007,
    "open3_opp": 0.007
  },
  "rewards_extra": {
    "imm_win_self": 0.03,
    "imm_win_opp": 0.03,
    "fork_self": 0.03,
    "fork_opp": 0.03,
    "run_self": 0.01,
    "run_opp": 0.01,
    "open2_self": 0.005,
    "open2_opp": 0.005,
    "closed4_breaker": 0.03,
    "jack_remove_impact": 0.02,
    "wild_efficiency": 0.08,
    "hot_control_self": 0.002,
    "hot_control_opp": 0.002,
    "center_control": 0.005,
    "corner_synergy": 0.005,
    "coverage_cut": 0.005,
    "redundancy_penalty": 0.02,
    "protected_chips": 0.01,
    "jack_vuln_self": 0.01,
    "jack_vuln_opp": 0.01,
    "blunder": 0.05,
    "mobility": 0.0005,
    "discard_ev": 0.005,
    "early_finish_bonus": 0.25
  },
  "shaping": {
    "use_potential": true,
    "per_term_clip": 0.25,
    "global_clip": 0.5,
    "anneal_total_env_steps": 200000,
    "anneal_min_scale": 0.0
  },
  "performance": {
    "use_torch_features": true,
    "device": "cuda",
    "dtype": "float32"
  }
}